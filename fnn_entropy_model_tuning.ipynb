{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# <font color= 'blue'> Deep Learning\n",
        "# <font color='blue'> Project 1 </font>\n",
        "\n",
        "## <font color= 'blue'> Fully Connected Neural Network\n",
        "Mehek Niwas || MTH 4320 Fall 2023"
      ],
      "metadata": {
        "id": "hl2zoLiZN4pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cannot use softmax with a binary classification."
      ],
      "metadata": {
        "id": "IPohOhOS41ln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cannot calculate entropy for ReLU, therefore ReLU should not be the function for the output layer when loss must be calculated"
      ],
      "metadata": {
        "id": "V7yI_B9wZg9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use k-fold cross-validation?"
      ],
      "metadata": {
        "id": "tayqk9hY8xxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color= 'blue'> Goals\n",
        "*The goals of the project:*\n",
        "\n",
        "1) Use a fully-connected feed forward neural network to predict the survival status of patients given a bone marrow transplant.\n",
        "\n",
        "2) Calculation entropy for each layer of neural network for the best epoch, and calculate total entropy of each epoch"
      ],
      "metadata": {
        "id": "yBa_bvvaOEIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "!pip install ucimlrepo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNT_nbLggvz6",
        "outputId": "a020298b-c267-4239-c062-fda7c8fc025c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.10/dist-packages (0.0.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color= 'blue'> Dataset Preparation & Preprocessing\n",
        "## Bone Marrow Transplants in Children\n",
        "\n",
        "This dataset was obtained from from UCI Machine Learning Repository. Dataset has data on 187 bone transplant patients, with information such as DonorABO, Recipientage, PLTrecovery, and more in integer and 1-hot/binary form already.\n",
        "\n",
        "The total number of patients with all data was 142, so the data from 45 patients had to be removed for a balanced dataset.\n",
        "\n",
        "Other adjustments:\n",
        "- switched \"disease\" categorial column to 1-hot encoding\n",
        "- removed \"ALL\" diseases binary column due to redundancy\n",
        "- removed \"recipientage10\" binary column due to redundancy with \"recipientage\" integer column\n",
        "- removed \"time to development of agvhd stage iii and iv\" due to redundancy with \"prescence of agvhd\" column. values for time to development of agvhd were very skewed with the values to represent no development (ex: 1,000,000 days) as well.\n",
        "- removed \"survival time\" integer column due to conflict with output for prediciting patient survival status\n"
      ],
      "metadata": {
        "id": "9No_XOFROP_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "import pandas as pd\n",
        "\n",
        "# fetch dataset --> from https://archive.ics.uci.edu/dataset/565/bone+marrow+transplant+children\n",
        "bone_marrow_transplant_children = fetch_ucirepo(id=565)\n",
        "\n",
        "# metadata\n",
        "print(bone_marrow_transplant_children.metadata)\n",
        "\n",
        "# variable information\n",
        "print(bone_marrow_transplant_children.variables)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXrQiQGA9St3",
        "outputId": "61472115-4c25-4969-e00e-5485e7b95476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'uci_id': 565, 'name': 'Bone marrow transplant: children', 'repository_url': 'https://archive.ics.uci.edu/dataset/565/bone+marrow+transplant+children', 'data_url': 'https://archive.ics.uci.edu/static/public/565/data.csv', 'abstract': 'The data set describes pediatric patients with several hematologic diseases, who were subject to the unmanipulated allogeneic unrelated donor hematopoietic stem cell transplantation.', 'area': 'Health and Medicine', 'tasks': ['Classification', 'Regression'], 'characteristics': ['Multivariate'], 'num_instances': 187, 'num_features': 36, 'feature_types': ['Integer', 'Real'], 'demographics': ['Gender', 'Age'], 'target_col': ['survival_status'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 2020, 'last_updated': 'Fri Nov 03 2023', 'dataset_doi': '10.24432/C5NP6Z', 'creators': ['Marek Sikora', 'Łukasz Wróbel', 'Adam Gudyś'], 'intro_paper': {'title': 'GuideR: a guided separate-and-conquer rule learning in classification, regression, and survival settings', 'authors': 'M. Sikora, Lukasz Wróbel, Adam Gudyś', 'published_in': 'Knowledge-Based Systems', 'year': 2018, 'url': 'https://arxiv.org/abs/1806.01579', 'doi': None}, 'additional_info': {'summary': \"The data set describes pediatric patients with several hematologic diseases: malignant disorders (i.a. acute lymphoblastic leukemia, acute myelogenous leukemia, chronic myelogenous leukemia, myelodysplastic syndrome) and nonmalignant cases (i.a. severe aplastic anemia, Fanconi anemia, with X-linked adrenoleukodystrophy). All patients were subject to the unmanipulated allogeneic unrelated donor hematopoietic stem cell transplantation. \\r\\n \\r\\nThe motivation of the study was to identify the most important factors influencing the success or failure of the transplantation procedure. In particular, the aim was to verify the hypothesis that increased dosage of CD34+ cells / kg extends overall survival time without simultaneous occurrence of undesirable events affecting patients' quality of life (KawÅ‚ak et al., 2010).\\r\\n\\r\\nThe data set has been used in our work concerning survival rules (WrÃ³bel et al., 2017) and user-guided rule induction (Sikora et al., 2019). The authors of the research on stem cell transplantation (KawÅ‚ak et al., 2010) who inspired our study also contributed to the set.\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'donor_age - Age of the donor at the time of hematopoietic stem cells apheresis\\r\\ndonor_age_below_35 - Is donor age less than 35 (yes, no)\\r\\ndonor_ABO - ABO blood group of the donor of hematopoietic stem cells (0, A, B, AB)\\r\\ndonor_CMV - Presence of cytomegalovirus infection in the donor of hematopoietic stem cells prior to transplantation (present, absent)\\r\\nrecipient_age - Age of the recipient of hematopoietic stem cells at the time of transplantation\\r\\nrecipient_age_below_10 - Is recipient age below 10 (yes, no)\\r\\nrecipient_age_int - Age of the recipient discretized to intervals (0,5], (5, 10], (10, 20]\\r\\nrecipient_gender - Gender of the recipient (female, male)\\r\\nrecipient_body_mass - Body mass of the recipient of hematopoietic stem cells at the time of the transplantation\\r\\nrecipient_ABO - ABO blood group of the recipient of hematopoietic stem cells (0, A, B, AB)\\r\\nrecipient_rh - Presence of the Rh factor on recipientâ€™s red blood cells (plus, minus)\\r\\nrecipient_CMV - Presence of cytomegalovirus infection in the donor of hematopoietic stem cells prior to transplantation (present, absent)\\r\\ndisease - Type of disease (ALL, AML, chronic, nonmalignant, lymphoma)\\r\\ndisease_group - Type of disease (malignant, nonmalignant)\\r\\ngender_match - Compatibility of the donor and recipient according to their gender (female to male, other)\\r\\nABO_match - Compatibility of the donor and the recipient of hematopoietic stem cells according to ABO blood group (matched, mismatched)\\r\\nCMV_status - Serological compatibility of the donor and the recipient of hematopoietic stem cells according to cytomegalovirus infection prior to transplantation (the higher the value, the lower the compatibility)\\r\\nHLA_match - Compatibility of antigens of the main histocompatibility complex of the donor and the recipient of hematopoietic stem cells (10/10, 9/10, 8/10, 7/10)\\r\\nHLA_mismatch - HLA matched or mismatched\\r\\nantigen - In how many antigens there is a difference between the donor and the recipient (0-3)\\r\\nallel - In how many allele there is a difference between the donor and the recipient (0-4)\\r\\nHLA_group_1 - The difference type between the donor and the recipient (HLA matched, one antigen, one allel, DRB1 cell, two allele or allel+antigen, two antigenes+allel, mismatched)\\r\\nrisk_group - Risk group (high, low)\\r\\nstem_cell_source - Source of hematopoietic stem cells (peripheral blood, bone marrow)\\r\\ntx_post_relapse - The second bone marrow transplantation after relapse (yes ,no)\\r\\nCD34_x1e6_per_kg - CD34kgx10d6 - CD34+ cell dose per kg of recipient body weight (10^6/kg)\\r\\nCD3_x1e8_per_kg - CD3+ cell dose per kg of recipient body weight (10^8/kg)\\r\\nCD3_to_CD34_ratio - CD3+ cell to CD34+ cell ratio\\r\\nANC_recovery - Neutrophils recovery defined as neutrophils count >0.5 x 10^9/L (yes, no)\\r\\ntime_to_ANC_recovery - Time in days to neutrophils recovery\\r\\nPLT_recovery - Platelet recovery defined as platelet count >50000/mm3 (yes, no)\\r\\ntime_to_PLT_recovery - Time in days to platelet recovery\\r\\nacute_GvHD_II_III_IV - Development of acute graft versus host disease stage II or III or IV (yes, no)\\r\\nacute_GvHD_III_IV - Development of acute graft versus host disease stage III or IV (yes, no)\\r\\ntime_to_acute_GvHD_III_IV - Time in days to development of acute graft versus host disease stage III or IV\\r\\nextensive_chronic_GvHD - Development of extensive chronic graft versus host disease (yes, no)\\r\\nrelapse - Relapse of the disease (yes, no)\\r\\nsurvival_time - Time of observation (if alive) or time to event (if dead) in days\\r\\nsurvival_status - Survival status (0 - alive, 1 - dead)', 'citation': None}}\n",
            "                    name     role         type demographic  \\\n",
            "0        Recipientgender  Feature       Binary      Gender   \n",
            "1         Stemcellsource  Feature       Binary        None   \n",
            "2               Donorage  Feature      Integer         Age   \n",
            "3             Donorage35  Feature       Binary         Age   \n",
            "4                   IIIV  Feature       Binary        None   \n",
            "5            Gendermatch  Feature       Binary      Gender   \n",
            "6               DonorABO  Feature  Categorical        None   \n",
            "7           RecipientABO  Feature  Categorical        None   \n",
            "8            RecipientRh  Feature       Binary        None   \n",
            "9               ABOmatch  Feature       Binary        None   \n",
            "10             CMVstatus  Feature  Categorical        None   \n",
            "11              DonorCMV  Feature       Binary        None   \n",
            "12          RecipientCMV  Feature       Binary        None   \n",
            "13               Disease  Feature  Categorical        None   \n",
            "14             Riskgroup  Feature       Binary        None   \n",
            "15         Txpostrelapse  Feature       Binary        None   \n",
            "16          Diseasegroup  Feature       Binary        None   \n",
            "17              HLAmatch  Feature  Categorical        None   \n",
            "18           HLAmismatch  Feature       Binary        None   \n",
            "19               Antigen  Feature  Categorical        None   \n",
            "20                Allele  Feature  Categorical        None   \n",
            "21                HLAgrI  Feature  Categorical        None   \n",
            "22          Recipientage  Feature      Integer         Age   \n",
            "23        Recipientage10  Feature       Binary         Age   \n",
            "24       Recipientageint  Feature  Categorical         Age   \n",
            "25               Relapse  Feature       Binary        None   \n",
            "26            aGvHDIIIIV  Feature       Binary        None   \n",
            "27              extcGvHD  Feature       Binary        None   \n",
            "28           CD34kgx10d6  Feature      Integer        None   \n",
            "29              CD3dCD34  Feature      Integer        None   \n",
            "30           CD3dkgx10d8  Feature      Integer        None   \n",
            "31             Rbodymass  Feature      Integer        None   \n",
            "32           ANCrecovery  Feature      Integer        None   \n",
            "33           PLTrecovery  Feature      Integer        None   \n",
            "34  time_to_aGvHD_III_IV  Feature      Integer        None   \n",
            "35         survival_time  Feature      Integer        None   \n",
            "36       survival_status   Target      Integer        None   \n",
            "\n",
            "                                          description    units missing_values  \n",
            "0                                Male - 1, Female - 0     None             no  \n",
            "1   Source of hematopoietic stem cells (Peripheral...     None             no  \n",
            "2   Age of the donor at the time of hematopoietic ...     None             no  \n",
            "3               Donor age <35 - 0, Donor age >=35 - 1     None             no  \n",
            "4   Development of acute graft versus host disease...     None             no  \n",
            "5   Compatibility of the donor and recipient accor...     None             no  \n",
            "6   ABO blood group of the donor of hematopoietic ...     None             no  \n",
            "7   ABO blood group of the recipient of hematopoie...     None            yes  \n",
            "8   Presence of the Rh factor on recipient s red b...     None            yes  \n",
            "9   Compatibility of the donor and the recipient o...     None            yes  \n",
            "10  Serological compatibility of the donor and the...     None            yes  \n",
            "11  resence of cytomegalovirus infection in the do...     None            yes  \n",
            "12  Presence of cytomegalovirus infection in the d...     None            yes  \n",
            "13  Type of disease (ALL,AML,chronic,nonmalignant,...     None             no  \n",
            "14                        High risk - 1, Low risk - 0     None             no  \n",
            "15  The second bone marrow transplantation after r...     None             no  \n",
            "16  Type of disease (malignant - 1, nonmalignant - 0)     None             no  \n",
            "17  Compatibility of antigens of the main histocom...     None             no  \n",
            "18                 HLA matched - 0, HL mismatched - 1     None             no  \n",
            "19  In how many antigens there is difference beetw...     None            yes  \n",
            "20  In how many allele there is difference beetwen...     None            yes  \n",
            "21  The difference type between the donor and the ...     None             no  \n",
            "22  Age of the recipient of hematopoietic stem cel...     None             no  \n",
            "23       Recipient age <10 - 0, Recipient age>=10 - 1     None             no  \n",
            "24  Recipient age in (0,5] - 0, (5, 10] - 1, (10, ...     None             no  \n",
            "25      Reoccurrence of the disease (No - 0, Yes - 1)     None             no  \n",
            "26  Development of acute graft versus host disease...     None             no  \n",
            "27  Development of extensive chronic graft versus ...     None            yes  \n",
            "28    CD34+ cell dose per kg of recipient body weight  10^6/kg             no  \n",
            "29                      CD3+ cell to CD34+ cell ratio     None            yes  \n",
            "30     CD3+ cell dose per kg of recipient body weight  10^8/kg            yes  \n",
            "31  Body mass of the recipient of hematopoietic st...     None            yes  \n",
            "32  Time to neutrophils recovery defined as neutro...     None             no  \n",
            "33  Time to platelet recovery defined as platelet ...     None             no  \n",
            "34  Time to development of acute graft versus host...     None             no  \n",
            "35  Time of observation (if alive) or time to even...     None             no  \n",
            "36              Survival status (0 - alive, 1 - dead)     None             no  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "    'uci_id': 20,\n",
        "    'name': 'Census Income',\n",
        "    'repository_url': 'https://archive.ics.uci.edu/dataset/20/census+income',\n",
        "    'data_url': 'https://archive.ics.uci.edu/static/public/20/data.csv',\n",
        "    'abstract': 'Predict whether income exceeds $50K/yr based on census data. Also known as Adult dataset.',\n",
        "    'area': 'Social Science',\n",
        "    'tasks': ['Classification'],\n",
        "    'characteristics': ['Multivariate'],\n",
        "    'num_instances': 48842,\n",
        "    'num_features': 14,\n",
        "    'feature_types': ['Categorical', 'Integer'],\n",
        "    'demographics': ['Age', 'Income', 'Education Level', 'Other', 'Race', 'Sex'],\n",
        "    'target_col': ['income'],\n",
        "    'index_col': None,\n",
        "    'has_missing_values': 'yes',\n",
        "    'missing_values_symbol': 'NaN',\n",
        "    'year_of_dataset_creation': 1996,\n",
        "    'last_updated': 'Thu Aug 10 2023',\n",
        "    'dataset_doi': '10.24432/C5GP7S',\n",
        "    'creators': ['Ron Kohavi'],\n",
        "    'intro_paper': None,\n",
        "    'additional_info': {\n",
        "        'summary': 'Extraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\\r\\n\\r\\nPrediction task is to determine whether a person makes over 50K a year.',\n",
        "        'purpose': None,\n",
        "        'funded_by': None,\n",
        "        'instances_represent': None,\n",
        "        'recommended_data_splits': None,\n",
        "        'sensitive_data': None,\n",
        "        'preprocessing_description': None,\n",
        "        'variable_info': 'Listing of attributes:\\r\\n\\r\\n>50K, <=50K.\\r\\n\\r\\nage: continuous.\\r\\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\\r\\nfnlwgt: continuous.\\r\\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\\r\\neducation-num: continuous.\\r\\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\\r\\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\\r\\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\\r\\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\\r\\nsex: Female, Male.\\r\\ncapital-gain: continuous.\\r\\ncapital-loss: continuous.\\r\\nhours-per-week: continuous.\\r\\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.',\n",
        "        'citation': None\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "6uTdJE_gbFKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb83cd3-9097-4779-ea40-9e3f35ed29ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'uci_id': 20,\n",
              " 'name': 'Census Income',\n",
              " 'repository_url': 'https://archive.ics.uci.edu/dataset/20/census+income',\n",
              " 'data_url': 'https://archive.ics.uci.edu/static/public/20/data.csv',\n",
              " 'abstract': 'Predict whether income exceeds $50K/yr based on census data. Also known as Adult dataset.',\n",
              " 'area': 'Social Science',\n",
              " 'tasks': ['Classification'],\n",
              " 'characteristics': ['Multivariate'],\n",
              " 'num_instances': 48842,\n",
              " 'num_features': 14,\n",
              " 'feature_types': ['Categorical', 'Integer'],\n",
              " 'demographics': ['Age', 'Income', 'Education Level', 'Other', 'Race', 'Sex'],\n",
              " 'target_col': ['income'],\n",
              " 'index_col': None,\n",
              " 'has_missing_values': 'yes',\n",
              " 'missing_values_symbol': 'NaN',\n",
              " 'year_of_dataset_creation': 1996,\n",
              " 'last_updated': 'Thu Aug 10 2023',\n",
              " 'dataset_doi': '10.24432/C5GP7S',\n",
              " 'creators': ['Ron Kohavi'],\n",
              " 'intro_paper': None,\n",
              " 'additional_info': {'summary': 'Extraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\\r\\n\\r\\nPrediction task is to determine whether a person makes over 50K a year.',\n",
              "  'purpose': None,\n",
              "  'funded_by': None,\n",
              "  'instances_represent': None,\n",
              "  'recommended_data_splits': None,\n",
              "  'sensitive_data': None,\n",
              "  'preprocessing_description': None,\n",
              "  'variable_info': 'Listing of attributes:\\r\\n\\r\\n>50K, <=50K.\\r\\n\\r\\nage: continuous.\\r\\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\\r\\nfnlwgt: continuous.\\r\\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\\r\\neducation-num: continuous.\\r\\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\\r\\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\\r\\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\\r\\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\\r\\nsex: Female, Male.\\r\\ncapital-gain: continuous.\\r\\ncapital-loss: continuous.\\r\\nhours-per-week: continuous.\\r\\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.',\n",
              "  'citation': None}}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data (as pandas dataframes)\n",
        "X = bone_marrow_transplant_children.data.features\n",
        "Y = bone_marrow_transplant_children.data.targets\n",
        "print(\"Original Dataset: \")\n",
        "print(\"Shape of Inputs: X =\", X.shape)\n",
        "print(\"Shape of Outputs: Y =\", Y.shape)\n",
        "print(\"\")\n",
        "\n",
        "# Convert the column name to lowercase to ensure case insensitivity\n",
        "X.columns = X.columns.str.lower()\n",
        "\n",
        "# code for dropping rows with any missing values\n",
        "# Capturing the index before dropping rows from the input DataFrame\n",
        "original_index = X.index\n",
        "\n",
        "# Dropping rows with missing values from the input DataFrame\n",
        "cleaned_X = X.dropna()\n",
        "\n",
        "# Filtering corresponding rows from the output DataFrame\n",
        "cleaned_Y = Y.loc[original_index.isin(cleaned_X.index)]\n",
        "cleaned_X.reset_index(drop=True, inplace = True) # resetting weird indexes\n",
        "\n",
        "# code for dropping identified columns upon data inspection by researcher\n",
        "cleaned_X = cleaned_X.drop(columns=['survival_time'])\n",
        "cleaned_X = cleaned_X.drop(['time_to_agvhd_iii_iv'], axis=1)\n",
        "cleaned_X = cleaned_X.drop(['recipientage10'], axis=1)\n",
        "\n",
        "cleaned_Y.reset_index(drop=True, inplace = True) # resetting weird indexes\n",
        "print(\"Redudant and conflicting features: \"); print(\" -- time to development of agvhd stage iii and iv\"); print(\" -- survival time\"); print(\" -- recipientage10\")\n",
        "print(\"\")\n",
        "print(\"After removal of redundant and conflicting features, and patients with missing data: \")\n",
        "print(\"New Shape of Inputs: X =\", cleaned_X.shape)\n",
        "print(\"New Shape of Outputs: Y =\", cleaned_Y.shape)\n",
        "print(\"\")\n",
        "\n",
        "# code for making the disease categories into 1 hot encoding\n",
        "\n",
        "# Get one hot encoding & Drop column 'Disease'\n",
        "one_hot = pd.get_dummies(cleaned_X['disease']) # i think the drop_first = True didnt work!!\n",
        "\n",
        "# Join one_hot and to original dataframe (cleaned_X)\n",
        "cleaned_X = cleaned_X.join(one_hot)\n",
        "\n",
        "\n",
        "# code for ALL in disease category\n",
        "# Define condition based on values in 'column_to_check'\n",
        "condition = cleaned_X['disease'].isin(['ALL'])  # condition\n",
        "\n",
        "# Modify values in specified columns based on the condition\n",
        "cleaned_X.loc[condition, ['AML', 'chronic', 'lymphoma', 'nonmalignant']] = 1\n",
        "\n",
        "cleaned_X = cleaned_X.drop('disease', axis=1)\n",
        "cleaned_X = cleaned_X.drop('ALL', axis=1)\n",
        "\n",
        "# making sure all columns are float type data!!\n",
        "cleaned_X = cleaned_X.astype('float64')\n",
        "cleaned_Y = cleaned_Y.astype('float64')\n",
        "\n",
        "print(\"After switch to 1-hot encoding for disease feature\")\n",
        "print(\"New Shape of Inputs: X =\", cleaned_X.shape)\n",
        "print(\"New Shape of Outputs: Y =\", cleaned_Y.shape)\n",
        "print(\"\")\n",
        "\n",
        "# Display the modified DataFrame\n",
        "print(\"Modified Inputs: \")\n",
        "print(cleaned_X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLOD4DNJg1ZB",
        "outputId": "f334256b-2da2-4bca-828d-5f838b008d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset: \n",
            "Shape of Inputs: X = (187, 36)\n",
            "Shape of Outputs: Y = (187, 1)\n",
            "\n",
            "Redudant and conflicting features: \n",
            " -- time to development of agvhd stage iii and iv\n",
            " -- survival time\n",
            " -- recipientage10\n",
            "\n",
            "After removal of redundant and conflicting features, and patients with missing data: \n",
            "New Shape of Inputs: X = (142, 33)\n",
            "New Shape of Outputs: Y = (142, 1)\n",
            "\n",
            "After switch to 1-hot encoding for disease feature\n",
            "New Shape of Inputs: X = (142, 36)\n",
            "New Shape of Outputs: Y = (142, 1)\n",
            "\n",
            "Modified Inputs: \n",
            "     recipientgender  stemcellsource   donorage  donorage35  iiiv  \\\n",
            "0                1.0             1.0  22.830137         0.0   1.0   \n",
            "1                1.0             0.0  23.342466         0.0   1.0   \n",
            "2                1.0             0.0  26.394521         0.0   1.0   \n",
            "3                0.0             1.0  33.358904         0.0   0.0   \n",
            "4                1.0             1.0  32.641096         0.0   0.0   \n",
            "..               ...             ...        ...         ...   ...   \n",
            "137              1.0             0.0  30.024658         0.0   1.0   \n",
            "138              1.0             1.0  37.575342         1.0   1.0   \n",
            "139              0.0             1.0  22.895890         0.0   0.0   \n",
            "140              0.0             1.0  27.347945         0.0   1.0   \n",
            "141              1.0             1.0  27.780822         0.0   1.0   \n",
            "\n",
            "     gendermatch  donorabo  recipientabo  recipientrh  abomatch  ...  \\\n",
            "0            0.0       1.0           1.0          1.0       0.0  ...   \n",
            "1            0.0      -1.0          -1.0          1.0       0.0  ...   \n",
            "2            0.0      -1.0          -1.0          1.0       0.0  ...   \n",
            "3            0.0       1.0           2.0          0.0       1.0  ...   \n",
            "4            0.0       2.0           0.0          1.0       1.0  ...   \n",
            "..           ...       ...           ...          ...       ...  ...   \n",
            "137          1.0       1.0           2.0          0.0       1.0  ...   \n",
            "138          0.0       1.0           1.0          0.0       0.0  ...   \n",
            "139          0.0       1.0           0.0          1.0       1.0  ...   \n",
            "140          0.0       1.0          -1.0          1.0       1.0  ...   \n",
            "141          0.0       1.0           0.0          1.0       1.0  ...   \n",
            "\n",
            "     cd34kgx10d6   cd3dcd34  cd3dkgx10d8  rbodymass  ancrecovery  pltrecovery  \\\n",
            "0           7.20   1.338760         5.38       35.0         19.0         51.0   \n",
            "1           4.50  11.078295         0.41       20.6         16.0         37.0   \n",
            "2           7.94  19.013230         0.42       23.4         23.0         20.0   \n",
            "3          51.85   3.972255        13.05        9.0         14.0         14.0   \n",
            "4          23.54   3.772555         6.24       20.5         15.0         14.0   \n",
            "..           ...        ...          ...        ...          ...          ...   \n",
            "137         8.11  16.326160         0.50       28.0         16.0        100.0   \n",
            "138        11.08   2.522750         4.39       44.0         15.0         22.0   \n",
            "139         4.64   1.038858         4.47       44.5         12.0         30.0   \n",
            "140         7.73   1.635559         4.73       33.0         16.0         16.0   \n",
            "141        15.41   8.077770         1.91       24.0         13.0         14.0   \n",
            "\n",
            "     AML  chronic  lymphoma  nonmalignant  \n",
            "0    1.0      1.0       1.0           1.0  \n",
            "1    1.0      1.0       1.0           1.0  \n",
            "2    1.0      1.0       1.0           1.0  \n",
            "3    0.0      1.0       0.0           0.0  \n",
            "4    0.0      0.0       0.0           1.0  \n",
            "..   ...      ...       ...           ...  \n",
            "137  1.0      1.0       1.0           1.0  \n",
            "138  0.0      0.0       1.0           0.0  \n",
            "139  1.0      0.0       0.0           0.0  \n",
            "140  0.0      0.0       0.0           1.0  \n",
            "141  0.0      1.0       0.0           0.0  \n",
            "\n",
            "[142 rows x 36 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# viewing modified input. looking for any categories that require preprocressing --> since unable to view more than 20 columns of pandas dataframe\n",
        "readX = cleaned_X.to_csv(\"inputsFinal.csv\")\n",
        "readY = cleaned_Y.to_csv(\"outputsFinal.csv\")\n"
      ],
      "metadata": {
        "id": "_-TWIOe2jDUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensuring all data types are float\n",
        "cleaned_X = cleaned_X.astype('float')\n",
        "cleaned_Y = cleaned_Y.astype('float')\n",
        "\n",
        "# Convert DataFrames to numpy array without column names --> prep for computations\n",
        "X_array = cleaned_X.values; print(X_array.shape)\n",
        "Y_array = cleaned_Y.values; print(Y_array.shape)\n",
        "# print(X_array, \" \", X_array.shape); print(); print(Y_array, \" \", Y_array.shape) # check arrays\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtylA52X8Hiy",
        "outputId": "2fd7cae5-994e-4631-d77a-05a95dd1d8ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(142, 36)\n",
            "(142, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color= 'blue'> Preliminary Model Architecture\n",
        "Model supports entropy calculation (log 2 --> in bits), activation functions (sigmoid vs. reLU), loss terms (sum of squares vs cross-entropy).\n",
        "First ran on XOR function to verify"
      ],
      "metadata": {
        "id": "pVxp9w6fOXwc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_fCb-kXww2M"
      },
      "outputs": [],
      "source": [
        "# aesthetics & other imports\n",
        "import numpy as np\n",
        "\n",
        "class color:\n",
        "  MAGENTA = '\\033[35;48m'\n",
        "  CYAN = '\\033[36;48m'\n",
        "  BOLD = '\\033[37;48m'\n",
        "  BLUE = '\\033[34;48m'\n",
        "  GREEN = '\\033[32;48m'\n",
        "  YELLOW = '\\033[33;48m'\n",
        "  RED = '\\033[91;48m'\n",
        "  BLACK = '\\033[30;48m'\n",
        "  BOLD = '\\033[1;30;48m' # (bolded BLACK)\n",
        "  UNDERLINE = '\\033[4;37;48m'\n",
        "  END = '\\033[1;37;0m'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will input a list of numbers of neurons to specify neurons in each layer (len(layers) = number of layers)\n",
        "# e.g. layers = [3, 6, 3] -- 3 layers: 3 neurons, 6 neurons, 3 neurons\n",
        "\n",
        "class FeedforwardNeuralNetwork:\n",
        "  # specified activation for output layer\n",
        "\n",
        "  def __init__(self, layers, alpha = 0.1, activation='sigmoid', output_activation='sigmoid', loss=\"sum-of-squares\"):\n",
        "    self.W = [] # list of matrices per layer\n",
        "    self.layers = layers\n",
        "    self.alpha = alpha\n",
        "    self.activation = activation\n",
        "    self.output_activation = output_activation\n",
        "    self.loss = loss\n",
        "    self.lossList = [] # list of loss per epoch\n",
        "    self.epochEntropies = [] # list of total entropies per epoch (all layers normalized and additive) --> should it be normalized? probably not. doesnt need to be. just total entropy.\n",
        "    self.layerEntropies = [] # list of lists of entropies per layer (goes by layer. 1st layer exempt) Note: ONLY FOR LAST EPOCH!!\n",
        "\n",
        "\n",
        "    # initialize the weights between layers (up to the next-to-last one) as random\n",
        "    for i in np.arange(0, len(layers) - 2): # np.arrange is the same thing as range\n",
        "      currentNeurons = layers[i]\n",
        "      nextNeurons = layers [i+1]\n",
        "      self.W.append(np.random.randn(currentNeurons + 1, nextNeurons + 1)) # +1 is for the bias neurons # random.randn --> mean=0, standard deviation=1\n",
        "\n",
        "    # initialize weights between the last two layers --> dont want bias for last layer!!\n",
        "    self.W.append(np.random.randn(layers[-2] + 1, layers[-1]))\n",
        "\n",
        "  def activate(self, layer, x): # layer starts with index 0 and includes input layer\n",
        "  # SOFTMAX IS STRICTLY ONLY IN OUTPUT ACTIVATION LAYER\n",
        "\n",
        "    if layer != (len(self.layers)-1):\n",
        "      if self.activation == 'sigmoid':\n",
        "        return 1.0 / (1 + np.exp(-x))\n",
        "      if self.activation == 'reLU':\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    else: # means we are in the output layer\n",
        "      if self.output_activation == 'sigmoid':\n",
        "        return 1.0 / (1 + np.exp(-x))\n",
        "      if self.output_activation == 'reLU':\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "  def activation_Derivative(self, layer, z): # layer starts with index 0 and includes input layer\n",
        "    # SOFTMAX IS STRICTLY ONLY IN OUTPUT ACTIVATION LAYER\n",
        "    if layer != (len(self.layers)-1):\n",
        "      if self.activation == 'sigmoid':\n",
        "        return z * (1 - z)\n",
        "      if self.activation == 'reLU':\n",
        "        return np.heaviside(z, 1)\n",
        "    else: # means we are in the output layer\n",
        "      if self.output_activation == 'sigmoid':\n",
        "        return z * (1 - z)\n",
        "      if self.output_activation == 'reLU':\n",
        "        return np.heaviside(z, 1)\n",
        "\n",
        "  def compute_entropy(self, layer, activationList): # layer has to include input layer and start with index 0\n",
        "    entropy = 0.0\n",
        "    activate_type = \"\"\n",
        "\n",
        "    if layer != (len(self.layers)-1): # if not last layer\n",
        "      activate_type = self.activation\n",
        "    else:\n",
        "      activate_type = self.output_activation\n",
        "\n",
        "    # if activate_type == 'reLU': --> entropy is 0, calculation not possible\n",
        "\n",
        "    if activate_type == 'sigmoid':\n",
        "      for i in range(len(activationList)): # want number of activations in activationList\n",
        "        entropy += (activationList[i] * np.log2(activationList[i])) # sum of (activations * log of activations). log2 for bits\n",
        "        entropy = -entropy # negative sign\n",
        "\n",
        "    return entropy\n",
        "\n",
        "\n",
        "  # fit the model\n",
        "  def fit(self, X, y, epochs = 10000, update = 1000):\n",
        "    # add a column of ones to the end of X\n",
        "    X = np.hstack((X, np.ones([X.shape[0],1]))) # --> also automatically turns X into an array & ignores columns\n",
        "\n",
        "    # list of lists per layer\n",
        "    epoch_totalList = []\n",
        "\n",
        "    for epoch in np.arange(0, epochs):\n",
        "      master_entropyList = [[] for i in range(len(self.layers)-1)] # entropyList for each layer to equal list of entropies for each epoch. -1 because first layer does not have calculated entropy since it is an input layer\n",
        "      # feed forward, backprop, and weight update\n",
        "      for (x, target) in zip(X, y):\n",
        "\n",
        "        # make a list of output activations from the first layer\n",
        "        A = [np.atleast_2d(x)] # currently --> (just the original x values)\n",
        "\n",
        "        # feed forward\n",
        "        for layer in np.arange(0, len(self.W)): # for each layer in weights list, which does not include the first nodes because those are just the inputs. those nodes are not really neurons, they only calculate inputs*weights, and then the activation that occurs will be applied by the second layer's neurons\n",
        "          neuronsNum = self.layers[layer+1]\n",
        "\n",
        "          # feed through one layer and apply sigmoid activation\n",
        "          z = A[layer].dot(self.W[layer]) # multiplying layer inputs with weights # .dot is the same as @ --> matrix multiplication\n",
        "          out = self.activate(layer+1, z) # activation function being applied\n",
        "\n",
        "          # add our network output to the list of activations --> append activation to A\n",
        "\n",
        "          A.append(out) # storing activation\n",
        "          insideOut = out[len(out)-1] # since out is a list of lists.\n",
        "          entropy_perLay_perInp = self.compute_entropy(layer+1, insideOut) # entropy per layer per input --> no normalization\n",
        "          master_entropyList[layer].append(entropy_perLay_perInp)\n",
        "          # calculating entropy for the layer\n",
        "\n",
        "        # backpropagation\n",
        "\n",
        "        error = A[-1] - target # sum of least squares error, where A[-1] is the activation from the last layer (output)\n",
        "        # only get error from last layer because it corresponds to the output/prediction, which can only be obtained from last layer?? (i think)\n",
        "\n",
        "\n",
        "        # term proportional to the gradient --> make a list to store the derivatives\n",
        "        last_lay_index = len(self.layers)-1\n",
        "        D = [error * self.activation_Derivative(last_lay_index, A[-1])] # can change this to use cross-entropy error instead of sum of squared errors\n",
        "\n",
        "        # cross-entropy only works for classification\n",
        "\n",
        "        # loop backwards over the layers to build up deltas --> backpropagate for errors\n",
        "        for layer in np.arange(len(A) - 2, 0, -1): # basicaly excludes input and output layer\n",
        "          delta = D[-1].dot(self.W[layer].T)\n",
        "          delta = delta * self.activation_Derivative(layer+1, A[layer])\n",
        "          D.append(delta)\n",
        "\n",
        "        # reverse the deltas since we looped in reverse\n",
        "        D = D[::-1] # now D contains all of the partial derivatives\n",
        "\n",
        "        # weight update\n",
        "        for layer in np.arange(0, len(self.W)):\n",
        "          self.W[layer] -= self.alpha * A[layer].T.dot(D[layer])\n",
        "\n",
        "      totalEpochEnt = 0\n",
        "      for i in range(len(master_entropyList)): # accessing master list --> i is layer #. layer starts at the second layer, since first layer is input layer (no calculation for input layer)\n",
        "        summationEnt = 0\n",
        "\n",
        "        for j in range(len(master_entropyList[i])): # j is input # for ith layer. trying to compile all input entropies and average them for each layer (layer goes by i)\n",
        "\n",
        "          entropy_perLayList = master_entropyList[i]\n",
        "          summationEnt += entropy_perLayList[j]\n",
        "\n",
        "          if j == (len(entropy_perLayList)-1):\n",
        "\n",
        "            summationAvg = summationEnt / len(entropy_perLayList) # len(entropy_perLayList = number of x's (input rows) in X)\n",
        "            totalEpochEnt += summationAvg\n",
        "            master_entropyList[i] = summationAvg/(self.layers[i+1]) # dividing total entropy for particular layer by its number of neurons\n",
        "\n",
        "      if epoch == epochs-1:\n",
        "        self.layerEntropies = master_entropyList # storing last and final epoch's individual layer entropies\n",
        "        print(\"Note: The following entropies stated are the averages found only in the last epoch, which is epoch \", epoch)\n",
        "        print(\"If the entropy is 0, then most likely the entropy is not able to be calculated for the specific activation function used in the layer\")\n",
        "        for i in range(len(master_entropyList)):\n",
        "          print(color.MAGENTA, \"Average total entropy for layer (Normalized)\", i+2, \": \", master_entropyList[i], color.END, \"Info: Layer had \", self.layers[i+1], \" neurons. \")\n",
        "\n",
        "      self.epochEntropies.append(totalEpochEnt) # storing total entropy from all layers from epoch (average across inputs)\n",
        "\n",
        "      # print a status update\n",
        "      if (epoch + 1) % update == 0:\n",
        "        loss = self.computeLoss(X,y)\n",
        "        self.lossList.append(loss)\n",
        "        print(color.BLUE, 'EPOCH =', epoch + 1, 'LOSS = ', loss, color.END)\n",
        "\n",
        "  def predict(self, X, addOnes = True):\n",
        "    # initialize data, be sure it's the right dimension\n",
        "    p = np.atleast_2d(X)\n",
        "\n",
        "    # add a column of 1s for bias\n",
        "    if addOnes: # in case we already did appending before we called the predict function!\n",
        "      p = np.hstack((p, np.ones([X.shape[0],1])))\n",
        "\n",
        "    # feed forward!\n",
        "    for layer in np.arange(0, len(self.W)):\n",
        "      p = self.activate(layer+1, np.dot(p, self.W[layer])) # only need final output, dont need to store all of it\n",
        "      # final p will be the final predictions that were outputted from the last layer of the neural network\n",
        "\n",
        "    # return the predictions\n",
        "    return p\n",
        "\n",
        "  def computeLoss(self, X, y):\n",
        "    # initialize data, be sure it's the right dimension\n",
        "    y = np.atleast_2d(y) # might not need this\n",
        "\n",
        "    # feed the datapoints through the network to get predicted outputs\n",
        "    predictions = self.predict(X, addOnes = False)\n",
        "\n",
        "    # compute the sum of squared errors loss function\n",
        "    if(self.loss == \"sum-of-squares\"):\n",
        "      loss = np.sum((predictions - y)**2) / 2.0\n",
        "\n",
        "    # compute cross entropy loss function. it is more effective because it uses a log so it more heavily penalizes bad predictions\n",
        "    if(self.loss == \"cross-entropy\"):\n",
        "      loss = np.sum(np.nan_to_num(-y * np.log(predictions) - (1 - y) * np.log(1 - predictions)))\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "3fPCWDIW8vSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color= 'blue'> Finally doing a preliminary test on the data!!"
      ],
      "metadata": {
        "id": "nSi26Vr2sCdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "# split the data into training and test sets (randomized)\n",
        "trainX, testX, trainY, testY = train_test_split(X_array, Y_array, test_size = 0.25, random_state = 1)\n",
        "\n",
        "trainX = scale(trainX)\n",
        "testX = scale(testX) # have to scale to make sure parameters are scaled the same in both --> question: how is it being scaled?\n",
        "\n",
        "print(\"trainInputs shape: \", trainX.shape); print(\"trainOutputs shape: \", trainY.shape)\n",
        "print(\"testInputs shape: \", testX.shape); print(\"testInputs shape: \", testY.shape)\n"
      ],
      "metadata": {
        "id": "D_duh6bJb38s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d308b0d4-206e-4591-e8f0-0cdd27e79c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainInputs shape:  (106, 36)\n",
            "trainOutputs shape:  (106, 1)\n",
            "testInputs shape:  (36, 36)\n",
            "testInputs shape:  (36, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# for relu with output activation as sigmoid, it is quite suspicious how the output is either 0.5 or lower. also inf is being encountered upon many epochs because loss becomes exponential"
      ],
      "metadata": {
        "id": "2rZwu9proFj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# mini-reLU\n",
        "print(\"for reLU: \")\n",
        "model = FeedforwardNeuralNetwork(layers = [36, 1, 1], loss='cross-entropy', activation='reLU', output_activation='sigmoid') # 3 layers, have to have 37 input \"nodes\" in first layer, and 1 output \"nodes\" in last layer\n",
        "model.fit(trainX, trainY, 200, 50)\n",
        "#model.fit(trainX, trainY, 2, 1) # making sure there are no overflow errors\n",
        "print(\"\")\n",
        "model.predict(testX)\n",
        "predictions = model.predict(testX)\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "\n",
        "print(\"non interpreted version: \")\n",
        "print(predictions)\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i] < 0.50000000000001: # works if interpretation is < 0.5\n",
        "    predictions[i] = 0\n",
        "  else:\n",
        "    predictions[i] = 1\n",
        "print(\"interpreted version: \")\n",
        "print(predictions)\n",
        "\n",
        "#print(predictions)\n",
        "print()\n",
        "print(\"the true values are: \")\n",
        "print(testY)\n",
        "print(\"Seeing if predictions worked: \")\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i] == testY[i]:\n",
        "    print(\"True\")\n",
        "  else:\n",
        "    print(\"False, answer was: \", testY[i])\n",
        "\n",
        "# ReLU does not work well with cross-entropy loss because it does not always output a probability for activation\n"
      ],
      "metadata": {
        "id": "t2lMhYJQwyAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd90b570-3530-4c1b-bf41-cb7d10573ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for reLU: \n",
            "\u001b[34;48m EPOCH = 50 LOSS =  1453.9171801597324 \u001b[1;37;0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-fe06c17386fc>:191: RuntimeWarning: divide by zero encountered in log\n",
            "  loss = np.sum(np.nan_to_num(-y * np.log(predictions) - (1 - y) * np.log(1 - predictions)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34;48m EPOCH = 100 LOSS =  1.7976931348623157e+308 \u001b[1;37;0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-fe06c17386fc>:191: RuntimeWarning: invalid value encountered in multiply\n",
            "  loss = np.sum(np.nan_to_num(-y * np.log(predictions) - (1 - y) * np.log(1 - predictions)))\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34;48m EPOCH = 150 LOSS =  inf \u001b[1;37;0m\n",
            "Note: The following entropies stated are the averages found only in the last epoch, which is epoch  199\n",
            "If the entropy is 0, then most likely the entropy is not able to be calculated for the specific activation function used in the layer\n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 2 :  0.0 \u001b[1;37;0m Info: Layer had  1  neurons. \n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 3 :  0.06018984962076109 \u001b[1;37;0m Info: Layer had  1  neurons. \n",
            "\u001b[34;48m EPOCH = 200 LOSS =  inf \u001b[1;37;0m\n",
            "\n",
            "\n",
            "non interpreted version: \n",
            "[[9.99999956e-01]\n",
            " [3.24038114e-22]\n",
            " [3.46185298e-34]\n",
            " [1.00000000e+00]\n",
            " [1.00000000e+00]\n",
            " [1.00000000e+00]\n",
            " [6.83149902e-30]\n",
            " [1.00000000e+00]\n",
            " [1.98190208e-54]\n",
            " [6.11190948e-13]\n",
            " [2.72456067e-07]\n",
            " [9.99941721e-01]\n",
            " [9.67500159e-03]\n",
            " [3.88023993e-18]\n",
            " [9.99999230e-01]\n",
            " [3.79903918e-09]\n",
            " [1.65096657e-29]\n",
            " [1.00000000e+00]\n",
            " [1.67678048e-29]\n",
            " [9.99989528e-01]\n",
            " [1.37082086e-15]\n",
            " [2.53320346e-48]\n",
            " [1.61835151e-18]\n",
            " [3.83786609e-05]\n",
            " [1.30029092e-26]\n",
            " [1.00000000e+00]\n",
            " [6.61392106e-25]\n",
            " [5.00000000e-01]\n",
            " [1.95679954e-83]\n",
            " [9.98911632e-01]\n",
            " [9.99999998e-01]\n",
            " [6.74762331e-01]\n",
            " [1.70519656e-07]\n",
            " [6.54444301e-11]\n",
            " [9.99536037e-01]\n",
            " [9.99999697e-01]]\n",
            "interpreted version: \n",
            "[[1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]]\n",
            "\n",
            "the true values are: \n",
            "[[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]]\n",
            "Seeing if predictions worked: \n",
            "False, answer was:  [0.]\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [0.]\n",
            "False, answer was:  [0.]\n",
            "False, answer was:  [0.]\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [0.]\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [0.]\n",
            "True\n",
            "True\n",
            "False, answer was:  [0.]\n",
            "True\n",
            "False, answer was:  [0.]\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "True\n",
            "False, answer was:  [0.]\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [0.]\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [0.]\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CROSS ENTROPY LOSS DOES NOT WORK WITH reLU !!!\n",
        "# mini-reLU\n",
        "print(\"for reLU: \")\n",
        "model = FeedforwardNeuralNetwork(layers = [36, 1, 1], loss='sum-of-squares', activation='reLU', output_activation='sigmoid') # 3 layers, have to have 37 input \"nodes\" in first layer, and 1 output \"nodes\" in last layer\n",
        "model.fit(trainX, trainY, 2, 1)\n",
        "print(\"\")\n",
        "\n",
        "# print(testX)\n",
        "predictions = model.predict(testX)\n",
        "print(\"non interpreted version: \")\n",
        "print(predictions)\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i] < 0.50000000001:\n",
        "    predictions[i] = 0\n",
        "  else:\n",
        "    predictions[i] = 1\n",
        "print(\"interpreted version: \")\n",
        "print(predictions)\n",
        "\n",
        "#print(predictions)\n",
        "print()\n",
        "print(\"the true values are: \")\n",
        "print(testY)\n",
        "print(\"Seeing if predictions worked: \")\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i] == testY[i]:\n",
        "    print(\"True\")\n",
        "  else:\n",
        "    print(\"False, answer was: \", testY[i])\n",
        "\n",
        "# ReLU does not work well with cross-entropy loss because it does not always output a probability for activation\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggy2ZhlCXtxL",
        "outputId": "7071d618-c888-4fd0-dace-729bc29c9674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for reLU: \n",
            "\u001b[34;48m EPOCH = 1 LOSS =  12.50042669826295 \u001b[1;37;0m\n",
            "Note: The following entropies stated are the averages found only in the last epoch, which is epoch  1\n",
            "If the entropy is 0, then most likely the entropy is not able to be calculated for the specific activation function used in the layer\n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 2 :  0.0 \u001b[1;37;0m Info: Layer had  1  neurons. \n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 3 :  0.33296685863936853 \u001b[1;37;0m Info: Layer had  1  neurons. \n",
            "\u001b[34;48m EPOCH = 2 LOSS =  14.372622456280506 \u001b[1;37;0m\n",
            "\n",
            "non interpreted version: \n",
            "[[2.34742742e-02]\n",
            " [5.00000000e-01]\n",
            " [5.00000000e-01]\n",
            " [2.28785159e-01]\n",
            " [6.89842746e-03]\n",
            " [4.88635841e-03]\n",
            " [2.33239770e-01]\n",
            " [2.26428743e-01]\n",
            " [5.00000000e-01]\n",
            " [8.83955025e-03]\n",
            " [5.00000000e-01]\n",
            " [2.06656008e-02]\n",
            " [8.06599820e-04]\n",
            " [1.05490810e-01]\n",
            " [1.82070908e-01]\n",
            " [1.86225030e-03]\n",
            " [5.00000000e-01]\n",
            " [6.37915382e-04]\n",
            " [1.65426702e-03]\n",
            " [1.93098010e-02]\n",
            " [5.33830097e-03]\n",
            " [1.56935840e-02]\n",
            " [3.91833979e-01]\n",
            " [3.83571457e-01]\n",
            " [1.46004152e-03]\n",
            " [1.09166057e-04]\n",
            " [2.01639158e-01]\n",
            " [6.55602275e-02]\n",
            " [9.80067531e-02]\n",
            " [1.55075738e-01]\n",
            " [4.42642814e-03]\n",
            " [5.00000000e-01]\n",
            " [5.00000000e-01]\n",
            " [5.00000000e-01]\n",
            " [1.51326957e-03]\n",
            " [4.01978391e-02]]\n",
            "interpreted version: \n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "\n",
            "the true values are: \n",
            "[[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]]\n",
            "Seeing if predictions worked: \n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReLU with 4 layers --> need to see how to address vanishing / exploding gradients. overflow is being encountered at sigmoid output activation"
      ],
      "metadata": {
        "id": "5gmwkvDYkI5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CROSS ENTROPY LOSS DOES NOT WORK WITH reLU !!!\n",
        "# mini-reLU\n",
        "print(\"for reLU with 4 layers: \")\n",
        "model = FeedforwardNeuralNetwork(layers = [36, 18, 9, 1], loss='sum-of-squares', activation='reLU', output_activation='sigmoid') # 3 layers, have to have 37 input \"nodes\" in first layer, and 1 output \"nodes\" in last layer\n",
        "model.fit(trainX, trainY, 2, 1)\n",
        "print(\"\")\n",
        "\n",
        "# print(testX)\n",
        "predictions = model.predict(testX)\n",
        "print(\"non interpreted version: \")\n",
        "print(predictions)\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i] < 0.50000000001:\n",
        "    predictions[i] = 0\n",
        "  else:\n",
        "    predictions[i] = 1\n",
        "print(\"interpreted version: \")\n",
        "print(predictions)\n",
        "\n",
        "#print(predictions)\n",
        "print()\n",
        "print(\"the true values are: \")\n",
        "print(testY)\n",
        "print(\"Seeing if predictions worked: \")\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i] == testY[i]:\n",
        "    print(\"True\")\n",
        "  else:\n",
        "    print(\"False, answer was: \", testY[i])\n",
        "\n",
        "# ReLU does not work well with cross-entropy loss because it does not always output a probability for activation\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aa32d31-38b8-4e0a-aa65-18169d8489bb",
        "id": "ytkQFt8Zjrcy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for reLU with 4 layers: \n",
            "\u001b[34;48m EPOCH = 1 LOSS =  17.49999999990724 \u001b[1;37;0m\n",
            "Note: The following entropies stated are the averages found only in the last epoch, which is epoch  1\n",
            "If the entropy is 0, then most likely the entropy is not able to be calculated for the specific activation function used in the layer\n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 2 :  0.0 \u001b[1;37;0m Info: Layer had  18  neurons. \n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 3 :  0.0 \u001b[1;37;0m Info: Layer had  9  neurons. \n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 4 :  nan \u001b[1;37;0m Info: Layer had  1  neurons. \n",
            "\u001b[34;48m EPOCH = 2 LOSS =  17.49999999990789 \u001b[1;37;0m\n",
            "\n",
            "non interpreted version: \n",
            "[[1.06094966e-064]\n",
            " [2.20654307e-040]\n",
            " [2.10468400e-234]\n",
            " [0.00000000e+000]\n",
            " [0.00000000e+000]\n",
            " [0.00000000e+000]\n",
            " [1.18658106e-164]\n",
            " [0.00000000e+000]\n",
            " [0.00000000e+000]\n",
            " [2.02499902e-193]\n",
            " [1.41036203e-055]\n",
            " [0.00000000e+000]\n",
            " [0.00000000e+000]\n",
            " [3.11175678e-289]\n",
            " [2.85160668e-276]\n",
            " [1.26917434e-163]\n",
            " [0.00000000e+000]\n",
            " [1.09034660e-101]\n",
            " [0.00000000e+000]\n",
            " [6.17719457e-236]\n",
            " [7.81877169e-175]\n",
            " [1.73276482e-101]\n",
            " [0.00000000e+000]\n",
            " [2.32308772e-178]\n",
            " [0.00000000e+000]\n",
            " [1.13475426e-131]\n",
            " [0.00000000e+000]\n",
            " [8.58798669e-208]\n",
            " [0.00000000e+000]\n",
            " [0.00000000e+000]\n",
            " [9.46157305e-143]\n",
            " [0.00000000e+000]\n",
            " [2.26032632e-037]\n",
            " [0.00000000e+000]\n",
            " [1.71742453e-132]\n",
            " [2.20955978e-219]]\n",
            "interpreted version: \n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "\n",
            "the true values are: \n",
            "[[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]]\n",
            "Seeing if predictions worked: \n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [1.]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-fe06c17386fc>:39: RuntimeWarning: overflow encountered in exp\n",
            "  return 1.0 / (1 + np.exp(-x))\n",
            "<ipython-input-38-fe06c17386fc>:69: RuntimeWarning: divide by zero encountered in log2\n",
            "  entropy += (activationList[i] * np.log2(activationList[i])) # sum of (activations * log of activations). log2 for bits\n",
            "<ipython-input-38-fe06c17386fc>:69: RuntimeWarning: invalid value encountered in scalar multiply\n",
            "  entropy += (activationList[i] * np.log2(activationList[i])) # sum of (activations * log of activations). log2 for bits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# doing calculations with sigmoid!!"
      ],
      "metadata": {
        "id": "rRBfljeMbz2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"for basic network: \")\n",
        "# LOSS --> SUM OF SQUARES\n",
        "# SIGMOID ACTIVATIONS AND SIGMOID OUTPUT ACTIVATIONS\n",
        "model = FeedforwardNeuralNetwork(layers = [36, 1, 1]) # 3 layers, have to have 37 input \"nodes\" in first layer, and 1 output \"nodes\" in last layer\n",
        "model.fit(trainX, trainY, 1000, 100) # making sure there are no overflow errors\n",
        "print(\"\")\n",
        "\n",
        "print(testX)\n",
        "predictions = model.predict(testX)\n",
        "print(\"non interpreted version: \")\n",
        "print(predictions)\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i] < 0.5:\n",
        "    predictions[i] = 0\n",
        "  else:\n",
        "    predictions[i] = 1\n",
        "print(\"interpreted version: \")\n",
        "print(predictions)\n",
        "\n",
        "#print(predictions)\n",
        "print()\n",
        "print(\"the true values are: \")\n",
        "print(testY)\n",
        "print(\"Seeing if predictions worked: \")\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i] == testY[i]:\n",
        "    print(\"True\")\n",
        "  else:\n",
        "    print(\"False, answer was: \", testY[i])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_sdah9D54dD",
        "outputId": "53a3cce7-d1fb-45ed-d169-98bd27ff3ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for basic network: \n",
            "\u001b[34;48m EPOCH = 100 LOSS =  9.359234811338407 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 200 LOSS =  6.975886518954928 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 300 LOSS =  6.484089065867445 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 400 LOSS =  6.371906276104341 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 500 LOSS =  6.317212572130562 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 600 LOSS =  6.27219729500222 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 700 LOSS =  6.222197801234125 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 800 LOSS =  5.5238863040123665 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 900 LOSS =  5.418052579250774 \u001b[1;37;0m\n",
            "Note: The following entropies stated are the averages found only in the last epoch, which is epoch  999\n",
            "If the entropy is 0, then most likely the entropy is not able to be calculated for the specific activation function used in the layer\n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 2 :  0.053361545948121175 \u001b[1;37;0m Info: Layer had  1  neurons. \n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 3 :  0.23511621395485127 \u001b[1;37;0m Info: Layer had  1  neurons. \n",
            "\u001b[34;48m EPOCH = 1000 LOSS =  5.212303083779949 \u001b[1;37;0m\n",
            "\n",
            "[[ 0.75180941  0.35355339  0.81377978 ... -1.18321596 -0.75180941\n",
            "   0.79772404]\n",
            " [ 0.75180941  0.35355339 -1.561873   ... -1.18321596 -0.75180941\n",
            "  -1.25356634]\n",
            " [ 0.75180941  0.35355339  0.12207873 ...  0.84515425  1.33012434\n",
            "   0.79772404]\n",
            " ...\n",
            " [ 0.75180941  0.35355339 -1.46064836 ...  0.84515425  1.33012434\n",
            "   0.79772404]\n",
            " [ 0.75180941  0.35355339  1.36082972 ... -1.18321596 -0.75180941\n",
            "  -1.25356634]\n",
            " [-1.33012434  0.35355339  0.78097556 ... -1.18321596 -0.75180941\n",
            "   0.79772404]]\n",
            "non interpreted version: \n",
            "[[0.01434823]\n",
            " [0.9557012 ]\n",
            " [0.21335898]\n",
            " [0.0211934 ]\n",
            " [0.02091435]\n",
            " [0.02260174]\n",
            " [0.47348695]\n",
            " [0.0210366 ]\n",
            " [0.27662279]\n",
            " [0.58449092]\n",
            " [0.93848065]\n",
            " [0.23332794]\n",
            " [0.01894879]\n",
            " [0.44839982]\n",
            " [0.92575629]\n",
            " [0.081189  ]\n",
            " [0.20168352]\n",
            " [0.01442885]\n",
            " [0.23615944]\n",
            " [0.01457991]\n",
            " [0.21204206]\n",
            " [0.49993177]\n",
            " [0.0972988 ]\n",
            " [0.01969595]\n",
            " [0.7190339 ]\n",
            " [0.23641969]\n",
            " [0.11539708]\n",
            " [0.84934367]\n",
            " [0.38491521]\n",
            " [0.01430003]\n",
            " [0.06999986]\n",
            " [0.72970656]\n",
            " [0.62144662]\n",
            " [0.95169962]\n",
            " [0.23846061]\n",
            " [0.95442801]]\n",
            "interpreted version: \n",
            "[[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]]\n",
            "\n",
            "the true values are: \n",
            "[[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]]\n",
            "Seeing if predictions worked: \n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [0.]\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [0.]\n",
            "True\n",
            "True\n",
            "False, answer was:  [0.]\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"for 4 layers basic neural network: \")\n",
        "print(\"all sigmoid activations and sum of squares loss\")\n",
        "# LOSS --> SUM OF SQUARES\n",
        "# SIGMOID ACTIVATIONS AND SIGMOID OUTPUT ACTIVATIONS\n",
        "model = FeedforwardNeuralNetwork(layers = [36, 18, 9,  1]) # 3 layers, have to have 37 input \"nodes\" in first layer, and 1 output \"nodes\" in last layer\n",
        "model.fit(trainX, trainY, 500, 100) # making sure there are no overflow errors\n",
        "print(\"\")\n",
        "\n",
        "# print(testX)\n",
        "predictions = model.predict(testX)\n",
        "print(\"non interpreted version: \")\n",
        "print(predictions)\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i] < 0.5:\n",
        "    predictions[i] = 0\n",
        "  else:\n",
        "    predictions[i] = 1\n",
        "\n",
        "#print(predictions)\n",
        "print()\n",
        "#print(\"the true values are: \")\n",
        "print(testY)\n",
        "print(\"Seeing if predictions worked: \")\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i] == testY[i]:\n",
        "    print(\"True\")\n",
        "  else:\n",
        "    print(\"False, answer was: \", testY[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgZYcfStBGQr",
        "outputId": "eb8c1ffc-d950-4ab0-812e-5e386434f703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for 4 layers basic neural network: \n",
            "\u001b[34;48m EPOCH = 100 LOSS =  0.2116806674020649 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 200 LOSS =  0.0626595321399891 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 300 LOSS =  0.03436943769347317 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 400 LOSS =  0.023131917230458453 \u001b[1;37;0m\n",
            "Note: The following entropies stated are the averages found only in the last epoch, which is epoch  499\n",
            "If the entropy is 0, then most likely the entropy is not able to be calculated for the specific activation function used in the layer\n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 2 :  0.006795591680520307 \u001b[1;37;0m Info: Layer had  18  neurons. \n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 3 :  -0.002644607537765477 \u001b[1;37;0m Info: Layer had  9  neurons. \n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 4 :  0.05856906477004634 \u001b[1;37;0m Info: Layer had  1  neurons. \n",
            "\u001b[34;48m EPOCH = 500 LOSS =  0.017235085729944196 \u001b[1;37;0m\n",
            "\n",
            "[[ 0.75180941  0.35355339  0.81377978 ... -1.18321596 -0.75180941\n",
            "   0.79772404]\n",
            " [ 0.75180941  0.35355339 -1.561873   ... -1.18321596 -0.75180941\n",
            "  -1.25356634]\n",
            " [ 0.75180941  0.35355339  0.12207873 ...  0.84515425  1.33012434\n",
            "   0.79772404]\n",
            " ...\n",
            " [ 0.75180941  0.35355339 -1.46064836 ...  0.84515425  1.33012434\n",
            "   0.79772404]\n",
            " [ 0.75180941  0.35355339  1.36082972 ... -1.18321596 -0.75180941\n",
            "  -1.25356634]\n",
            " [-1.33012434  0.35355339  0.78097556 ... -1.18321596 -0.75180941\n",
            "   0.79772404]]\n",
            "non interpreted version: \n",
            "[[0.01162459]\n",
            " [0.99357228]\n",
            " [0.01542822]\n",
            " [0.01896337]\n",
            " [0.92850196]\n",
            " [0.13458811]\n",
            " [0.00122474]\n",
            " [0.02004905]\n",
            " [0.93559958]\n",
            " [0.0148557 ]\n",
            " [0.99099248]\n",
            " [0.00127334]\n",
            " [0.85161859]\n",
            " [0.77144956]\n",
            " [0.48002584]\n",
            " [0.0022442 ]\n",
            " [0.50318249]\n",
            " [0.00949102]\n",
            " [0.49210686]\n",
            " [0.04709154]\n",
            " [0.00365004]\n",
            " [0.02024772]\n",
            " [0.01081372]\n",
            " [0.00452166]\n",
            " [0.6718358 ]\n",
            " [0.17232695]\n",
            " [0.09519934]\n",
            " [0.00335536]\n",
            " [0.61368012]\n",
            " [0.00146429]\n",
            " [0.00117287]\n",
            " [0.98161192]\n",
            " [0.10940706]\n",
            " [0.92815117]\n",
            " [0.99513603]\n",
            " [0.99044246]]\n",
            "\n",
            "[[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]]\n",
            "Seeing if predictions worked: \n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "False, answer was:  [0.]\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [0.]\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [0.]\n",
            "True\n",
            "True\n",
            "True\n",
            "False, answer was:  [0.]\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [0.]\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [0.]\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"for 5 layers basic neural network: \")\n",
        "print(\"all sigmoid activations & sum of squares loss\")\n",
        "# LOSS --> SUM OF SQUARES\n",
        "# SIGMOID ACTIVATIONS AND SIGMOID OUTPUT ACTIVATIONS\n",
        "model = FeedforwardNeuralNetwork(layers = [36, 20, 18, 9, 1]) # 3 layers, have to have 37 input \"nodes\" in first layer, and 1 output \"nodes\" in last layer\n",
        "model.fit(trainX, trainY, 500, 100) # making sure there are no overflow errors\n",
        "print(\"\")\n",
        "\n",
        "print(testX)\n",
        "predictions = model.predict(testX)\n",
        "print(\"non interpreted version: \")\n",
        "print(predictions)\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i] < 0.5:\n",
        "    predictions[i] = 0\n",
        "  else:\n",
        "    predictions[i] = 1\n",
        "\n",
        "#print(predictions)\n",
        "print()\n",
        "#print(\"the true values are: \")\n",
        "print(testY)\n",
        "print(\"Seeing if predictions worked: \")\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i] == testY[i]:\n",
        "    print(\"True\")\n",
        "  else:\n",
        "    print(\"False, answer was: \", testY[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d924cf1-1af8-45c5-e4c0-bac2c22d0161",
        "id": "rC-GzIF2BU4D"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for 5 layers basic neural network: \n",
            "\u001b[34;48m EPOCH = 100 LOSS =  0.2335769213340288 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 200 LOSS =  0.05357457077373107 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 300 LOSS =  0.02763618588854496 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 400 LOSS =  0.018092661896134767 \u001b[1;37;0m\n",
            "Note: The following entropies stated are the averages found only in the last epoch, which is epoch  499\n",
            "If the entropy is 0, then most likely the entropy is not able to be calculated for the specific activation function used in the layer\n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 2 :  0.017749325399780812 \u001b[1;37;0m Info: Layer had  20  neurons. \n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 3 :  0.03770047892091271 \u001b[1;37;0m Info: Layer had  18  neurons. \n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 4 :  -0.008410121696781386 \u001b[1;37;0m Info: Layer had  9  neurons. \n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 5 :  0.05746917421764185 \u001b[1;37;0m Info: Layer had  1  neurons. \n",
            "\u001b[34;48m EPOCH = 500 LOSS =  0.013270786753253843 \u001b[1;37;0m\n",
            "\n",
            "[[ 0.75180941  0.35355339  0.81377978 ... -1.18321596 -0.75180941\n",
            "   0.79772404]\n",
            " [ 0.75180941  0.35355339 -1.561873   ... -1.18321596 -0.75180941\n",
            "  -1.25356634]\n",
            " [ 0.75180941  0.35355339  0.12207873 ...  0.84515425  1.33012434\n",
            "   0.79772404]\n",
            " ...\n",
            " [ 0.75180941  0.35355339 -1.46064836 ...  0.84515425  1.33012434\n",
            "   0.79772404]\n",
            " [ 0.75180941  0.35355339  1.36082972 ... -1.18321596 -0.75180941\n",
            "  -1.25356634]\n",
            " [-1.33012434  0.35355339  0.78097556 ... -1.18321596 -0.75180941\n",
            "   0.79772404]]\n",
            "non interpreted version: \n",
            "[[0.00928963]\n",
            " [0.99833001]\n",
            " [0.04080884]\n",
            " [0.40044236]\n",
            " [0.07533213]\n",
            " [0.72630991]\n",
            " [0.95158527]\n",
            " [0.03982872]\n",
            " [0.03722943]\n",
            " [0.00772375]\n",
            " [0.45739507]\n",
            " [0.50148271]\n",
            " [0.08590801]\n",
            " [0.88393202]\n",
            " [0.13182199]\n",
            " [0.02698239]\n",
            " [0.00810286]\n",
            " [0.00636634]\n",
            " [0.16481181]\n",
            " [0.62859746]\n",
            " [0.10588147]\n",
            " [0.02564608]\n",
            " [0.0208899 ]\n",
            " [0.00570778]\n",
            " [0.52202971]\n",
            " [0.71574358]\n",
            " [0.05332654]\n",
            " [0.04404809]\n",
            " [0.05066508]\n",
            " [0.00686423]\n",
            " [0.00339705]\n",
            " [0.42168554]\n",
            " [0.99503436]\n",
            " [0.36467571]\n",
            " [0.30169185]\n",
            " [0.99509934]]\n",
            "\n",
            "[[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]]\n",
            "Seeing if predictions worked: \n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "False, answer was:  [0.]\n",
            "True\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "False, answer was:  [0.]\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [0.]\n",
            "False, answer was:  [0.]\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mini-cross entropy\n",
        "print(\"sigmoid for all layers and cross entropy loss: \")\n",
        "model = FeedforwardNeuralNetwork(layers = [36, 1, 1], loss='cross-entropy') # 3 layers, have to have 37 input \"nodes\" in first layer, and 1 output \"nodes\" in last layer\n",
        "model.fit(trainX, trainY, 10000, 1000) # making sure there are no overflow errors\n",
        "print(\"\")\n",
        "\n",
        "print(testX)\n",
        "predictions = model.predict(testX)\n",
        "print(\"non interpreted version: \")\n",
        "print(predictions)\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i] < 0.5:\n",
        "    predictions[i] = 0\n",
        "  else:\n",
        "    predictions[i] = 1\n",
        "\n",
        "#print(predictions)\n",
        "print()\n",
        "#print(\"the true values are: \")\n",
        "print(testY)\n",
        "print(\"Seeing if predictions worked: \")\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i] == testY[i]:\n",
        "    print(\"True\")\n",
        "  else:\n",
        "    print(\"False, answer was: \", testY[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_hs6SpHwt0S",
        "outputId": "96bbb37a-130c-4524-f6d8-33ab32719d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for cross entropy: \n",
            "\u001b[34;48m EPOCH = 1000 LOSS =  21.526664831747755 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 2000 LOSS =  16.603056188576623 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 3000 LOSS =  15.339225822273722 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 4000 LOSS =  14.231757047536824 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 5000 LOSS =  13.790998928825378 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 6000 LOSS =  13.63968219016571 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 7000 LOSS =  13.514362678630446 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 8000 LOSS =  12.855642106194708 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 9000 LOSS =  12.751601718329685 \u001b[1;37;0m\n",
            "Note: The following entropies stated are the averages found only in the last epoch, which is epoch  9999\n",
            "If the entropy is 0, then most likely the entropy is not able to be calculated for the specific activation function used in the layer\n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 2 :  0.021863304410239346 \u001b[1;37;0m Info: Layer had  1  neurons. \n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 3 :  0.11323789655101762 \u001b[1;37;0m Info: Layer had  1  neurons. \n",
            "\u001b[34;48m EPOCH = 10000 LOSS =  12.687502763245046 \u001b[1;37;0m\n",
            "\n",
            "[[ 0.75180941  0.35355339  0.81377978 ... -1.18321596 -0.75180941\n",
            "   0.79772404]\n",
            " [ 0.75180941  0.35355339 -1.561873   ... -1.18321596 -0.75180941\n",
            "  -1.25356634]\n",
            " [ 0.75180941  0.35355339  0.12207873 ...  0.84515425  1.33012434\n",
            "   0.79772404]\n",
            " ...\n",
            " [ 0.75180941  0.35355339 -1.46064836 ...  0.84515425  1.33012434\n",
            "   0.79772404]\n",
            " [ 0.75180941  0.35355339  1.36082972 ... -1.18321596 -0.75180941\n",
            "  -1.25356634]\n",
            " [-1.33012434  0.35355339  0.78097556 ... -1.18321596 -0.75180941\n",
            "   0.79772404]]\n",
            "non interpreted version: \n",
            "[[7.44039729e-05]\n",
            " [9.99872963e-01]\n",
            " [2.30116444e-05]\n",
            " [2.53254973e-03]\n",
            " [1.31693997e-01]\n",
            " [4.68263132e-01]\n",
            " [1.30193055e-01]\n",
            " [2.41017400e-05]\n",
            " [2.91568296e-01]\n",
            " [6.10445809e-05]\n",
            " [5.01683934e-01]\n",
            " [2.34113547e-04]\n",
            " [1.90267439e-05]\n",
            " [1.30510703e-01]\n",
            " [1.31986370e-01]\n",
            " [3.46776076e-05]\n",
            " [7.73414425e-05]\n",
            " [3.89756499e-05]\n",
            " [1.02005175e-01]\n",
            " [4.48021476e-05]\n",
            " [4.67452068e-01]\n",
            " [2.24109020e-05]\n",
            " [9.99684912e-01]\n",
            " [1.90265416e-05]\n",
            " [2.65889010e-01]\n",
            " [3.01052822e-03]\n",
            " [2.03619606e-05]\n",
            " [3.38159384e-02]\n",
            " [9.98878662e-01]\n",
            " [2.04525636e-05]\n",
            " [1.97047150e-05]\n",
            " [9.99872722e-01]\n",
            " [9.99872962e-01]\n",
            " [9.99777313e-01]\n",
            " [1.29263262e-01]\n",
            " [9.99872956e-01]]\n",
            "\n",
            "[[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]]\n",
            "Seeing if predictions worked: \n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "False, answer was:  [1.]\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data for exclusive OR function in coding\n",
        "# e.g. [0, 0] is a feature vector of inputs, and [0] is its corresponding output\n",
        "\n",
        "X1 = np.array([[0, 0], [1, 0], [0, 1], [1, 1]], dtype=float)\n",
        "Y1 = np.array([[0], [1], [1], [0]], dtype=float)\n"
      ],
      "metadata": {
        "id": "5dYNviWB8qqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FeedforwardNeuralNetwork(layers = [2, 2, 1])\n",
        "# neurons in first layer is determined by the dimension of the input which is 2\n",
        "# neurons in the hidden layer can be however many you want\n",
        "# neurons in the last layer is determined by the dimension of the output which is 1\n",
        "\n",
        "vars(model)\n"
      ],
      "metadata": {
        "id": "0NzRIS05874P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ed0041-2d50-43be-c470-aef203a288de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W': [array([[-0.46903939, -0.04988735, -1.47060113],\n",
              "         [ 0.53458613,  0.42089307, -2.03731276],\n",
              "         [ 1.28074014,  1.73331365, -1.38888897]]),\n",
              "  array([[ 2.42674676],\n",
              "         [-1.16603755],\n",
              "         [ 0.07984697]])],\n",
              " 'layers': [2, 2, 1],\n",
              " 'alpha': 0.1,\n",
              " 'activation': 'sigmoid',\n",
              " 'output_activation': 'sigmoid',\n",
              " 'loss': 'sum-of-squares',\n",
              " 'lossList': [],\n",
              " 'epochEntropies': [],\n",
              " 'layerEntropies': []}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.W[0].shape)\n",
        "print(model.W[1].shape)\n",
        "print(len(model.W)) # only 2 weight matrices even though there are 3 layers. because output of last layer is the prediction, it does not use another weight matrix.\n"
      ],
      "metadata": {
        "id": "9rC6nr97H7PS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d169cd-56e1-4e3f-b9a1-12c4096ac2c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 3)\n",
            "(3, 1)\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X1, Y1, 10000, 1000)\n",
        "\n",
        "# at 1:02:43 of lecture 8 - \"implementing backpropagation\"\n"
      ],
      "metadata": {
        "id": "3lEyCo43JKxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eeb1a27-59f0-47c4-b667-e4c8a796952d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34;48m EPOCH = 1000 LOSS =  0.1095410223979115 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 2000 LOSS =  0.034246885912749325 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 3000 LOSS =  0.01770399692754239 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 4000 LOSS =  0.011463656541701615 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 5000 LOSS =  0.008331880014235436 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 6000 LOSS =  0.006486014917034683 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 7000 LOSS =  0.005281815551439799 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 8000 LOSS =  0.0044396668389452875 \u001b[1;37;0m\n",
            "\u001b[34;48m EPOCH = 9000 LOSS =  0.0038202476008797294 \u001b[1;37;0m\n",
            "Note: The following entropies stated are the averages found only in the last epoch, which is epoch  9999\n",
            "If the entropy is 0, then most likely the entropy is not able to be calculated for the specific activation function used in the layer\n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 2 :  0.08614103679160928 \u001b[1;37;0m Info: Layer had  2  neurons. \n",
            "\u001b[35;48m Average total entropy for layer (Normalized) 3 :  0.12207909951840301 \u001b[1;37;0m Info: Layer had  1  neurons. \n",
            "\u001b[34;48m EPOCH = 10000 LOSS =  0.0033469119378694775 \u001b[1;37;0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(X1) # model works for classic XOR function!!\n"
      ],
      "metadata": {
        "id": "rkMWQdrHKPvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e7c9c1-ef97-446d-8b77-06ee782cc1ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.03761742],\n",
              "       [0.97455371],\n",
              "       [0.94938057],\n",
              "       [0.0454853 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    }
  ]
}